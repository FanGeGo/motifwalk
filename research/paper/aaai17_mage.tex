% Coding: utf-8
% Filename: aaai17_mage.tex
% Description: Paper submit to AAAI'17
% v0.0: File created. Motif-Aware Graph Embedding

% Standard settings by AAAI'17 authors' kit
\documentclass[letterpaper]{article}
\usepackage{aaai17}
\usepackage{times}
\usepackage{helvet}
\usepackage{courier}
\usepackage{graphics} % Using scalebox
\usepackage{dsfont} % Mathds
\usepackage{amsmath, amsthm, amssymb} % Align environment
\setlength{\pdfpagewidth}{8.5in}
\setlength{\pdfpageheight}{11in}
% PDF metadata
\pdfinfo{
    /Title (Motif-Aware Graph Embedding)
    /Author (Hoang Nguyen, Shun Nukui, Tsuyoshi Murata)
    /Keywords (Graph latent features, vertex representation, skipgram, graph motif)
}
% Section number
\setcounter{secnumdepth}{0}
% Title and authors
\title{Motif-Aware Graph Embedding}
\author{
    Hoang Nguyen \\
    Tokyo Institute of Technology \\
    hoangnt@net.c.titech.ac.jp \\
    \And 
    Shun Nukui \\ 
    Tokyo Institute of Techonology \\
    nukui.s@net.c.titech.ac.jp 
    \And 
    Tsuyoshi Murata \\
    Tokyo Institute of Techonology \\
    murata@c.titech.ac.jp 
}

%% Paper content
\begin{document}
    \maketitle

    \section{Abstract}
       Given a large complex graph, how can we have a lower dimension real-vector representations 
       of vertices that preserve structural information? Recent advancements in graph embedding 
       have adopted word embedding techniques and deep architectures to propose a feasible solution 
       to this question. However, most of these former researches considers the notion of ``neighborhood'' 
       by vertex adjacency only. In this paper, we propose a novel graph embedding algorithm that employs 
       motif structures into the latent vector representation learning process. Our algorithm learns 
       the graph latent representation by contrasting between different type of motif-biased random walk. 
       We showed that our algorithm yields more accurate embedding results compared to other existing 
       algorithms through various graph mining benchmark tasks.

    \setcounter{secnumdepth}{2}
    \section{Introduction}
        The graph (or network) data model is an useful tool for a wide range of disciplines, and
        it is essential to have a low dimensionality representation of a complex graph. In its
        simplest form, a graph is an ordered set of vertices connected by edges. Being simple
        and expressive, the graph-theoric approach has been applied in many scientic fields
        for untangling complex discrete structures. For example, the study conducted by 
        MP Van Den Heuvel suggested that the human brain functional network provides many new
        discoveries about our brain's organization \cite{BrainNetHeuvel}. The same approach
        for structural analysis can be found in other fields such as chemistry \cite{molecule}, 
        physics \cite{physicnet}, and sociology \cite{socialnet}. However, the graph analysis
        process often becomes intractable due to the complexity of the data. In such case,
        the graph usually contains several thousands to millions of vertices and edges.
        Therefore, it is desirable to have a compact latent representation of the graph while
        its statistical properties are retained. A \emph{high quality} latent representation
        of a graph can benefit machine learning algorithms in many ways. For instance, 
        the result and runtime of machine algorithm will be improved thanks to the low
        dimensionality of the data. On the other hand, instead of relying only on graph 
        mining algorithms to analyze some given data, researchers can also apply other
        machine learning algorithms on the learned laten respresentation to make predicitions.
        Conventionally, the latent representation learning procedure on graph is called
        \emph{graph embedding}.

        % Include fig 1 dimensionality reduction task explaination here.

        \emph{Learning} a high quality latent representation is a challenging task. Traditionally,
        dimensionality reduction technique such as PCA \cite{pca}, CCA \cite{cca}, and IsoMap 
        \cite{isomap} are used. Although these aforementioned techniques have a profound theoretical
        background \cite{++++++}, they are impractical due to computational drawbacks. To address
        the dimensionality problem, Peperozi et al.\ proposed Deepwalk - a Skipgram-based algorithm
        for graph embedding \cite{deepwalk}. Different from traditional linear algebra approach,
        Deepwalk learns the latent representation from probabilistic point of view. By treating
        a random series of vertices as if it is a sentence of words, Deepwalk adopts the operation
        of Skipgram model \cite{skipgram} to learn a vertex's latent representation. Following
        Deepwalk, other Skipgram-based graph embedding algorithms which aim to improve embedding
        quality were proposed \cite{GraRep, LINE, platenoid, node2vec}. However, these algorithms
        do not consider the intrinsic \emph{motif structure} of a graph. Therefore, the performance
        of these algorithms depend heavily on heuristic hyper-parameter tunning.

        % Include fig 2 sentence vs random walk.

        In this work, we propose an algorithm which controls both graph context and negative samples
        generation. The motif-aware context generation aims to emphasize the importance of local
        motif community, which is a strong indicator for true community in a graph. On the other hand,
        negative sampling is known to be the state of the art technique to estimate the normalization
        factor of a probabilistic model. Instead of sampling from a distorted unigram distrubution as
        suggested by Mikholov et al. \cite{skipgram}, we propose a motif-aware method to generate 
        negative samples from a graph. Generally, our algorithm, named Motif-Aware Graph Embedding (MAGE),
        has two following advantages:

        \begin{itemize}
            \setlength{\parskip}{0pt}
            \item Positive samples are generated using a motif-biased walk. This motif-aware context 
                generation procedure is backed by the hypothesis that vertices in the same motif
                are more likely to belong to the same community \cite{juremotif, harvardmotif}.
                By selecting the appropriate motif for each graph, we have a sensible way to control
                the context generated for embedding.
            \item Negative samples are also generated using a motif-biased walk. However, by choosing
                the \emph{opposing} motif to the characteristic motif of the graph, we have a concrete
                method to ``escape'' the motif community, which leads to a truely contrastive negative
                samples, hence better quality embedding.
        \end{itemize}

        Our algorithm is implemented on Keras \cite{keras} framework. The implementation and experimental
        results are available on Github. (TODO: ADD FOOTNOTE).

        The remaining of this paper is divided into 4 parts. Section 2 provides additional information
        about related work on graph embedding and graph motif. Section 3 presents our algorithm and
        experimental design. Section 4 and 5 discusses results and conclusion.

    \section{Related work}
        \subsection{Skipgram model}
            Representation learning has been one of the key to the success of machine learning 
            algorithms \cite{bengiorepreview}. In the context of natural language processing (NPL),
            representation learning becomes even more important as the data has a discrete, but
            high-dimension nature. To address the dimensionality problem in NLP, Mikholov et al.\ have
            proposed the Skipgram model \cite{skipgram}. Instead of maximizing the n-gram distribution
            as prior works, Skipgram maximize the occurence probability of context words given a
            target word. The softmax potential function for a context word given a target 
            word is given by:

            \begin{equation}
                \label{eq:model}
                \mbox{Pr} (v_c | v_t) = \frac{\exp{( \langle \omega_{v_c} ,  \omega_{v_t} \rangle )}}{\sum_{k \in V} \exp{( \langle \omega_{v_k} ,  \omega_{v_t} \rangle )}},
            \end{equation}
            
            \noindent
            where $ \langle \cdot ,  \cdot \rangle $ is vector dot product; $ v_c $ and $ v_t $ are the
            token for the context word and the target word respectively; $\omega_{v_t}$ is the embedding
            vector selected from the \emph{embedding matrix} by the token $v_t$; $\omega_{v_c}$ is the
            embedding vector selected from the \emph{context matrix} by the token $v_c$.

            Based on equation~\ref{eq:model}, the objective of Skipgram model is to maximize the following
            average log likelihood:

            \begin{equation}
                \label{eq:avgloglikelihood}
                \mathcal{O} = \max \left( \frac{1}{T} \sum_{t=1}^{T} \sum_{-c \leq j \leq c, j \neq 0} \log \mbox{Pr} (v_{t+j} | v_t) \right)
            \end{equation}

            The intrinsic instractable problem for softmax model is normalization factor computation. 
            Therefore, the normalization factor in equation~\ref{eq:model} needs to be estimated by 
            approximation techniques such as Hierachial Softmax \cite{hs} or Noise Contrastive estimation
            \cite{nce}. In their landmark paper, Mikolov et al.\  also proposed \emph{negative sampling} - a 
            simplified version of noise contrastive estimation. The log likelihood of under negative
            sampling scheme is given by:

            \begin{equation}
                \label{eq:objneg}
                \begin{aligned}
                \log \mbox{Pr} (v_c | v_t) & = \log \sigma( \langle \omega^{\scalebox{0.7}{nce}}_{v_c} , \omega^{\scalebox{0.7}{emb}}_{v_t} \rangle ) \\
                & + \sum_{i=1}^{k} \mathds{E}_{\omega_{v_i} \sim P_n(\omega)} \left[ \log \sigma( \langle \omega^{\scalebox{0.7}{nce}}_{v_i} , \omega^{\scalebox{0.7}{emb}}_{v_t} \rangle) \right]
                \end{aligned}
            \end{equation}
            

        \subsection{Context representation importance}
            The Skipgram model proposed by Mikolov et al. \cite{skipgram} is a powerful model 
            in natural language processing. Under the Distributional Hypothesis \cite{disthyp},
            the Skipgram model maximizes the occurent probability of context words given a target
            word.  
\end{document}
