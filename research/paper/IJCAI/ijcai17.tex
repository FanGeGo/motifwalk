%%%% ijcai17.tex

\typeout{Motif-Aware Graph Embeddings}

% These are the instructions for authors for IJCAI-17.
% They are the same as the ones for IJCAI-11 with superficical wording
%   changes only.


\documentclass{article}
% The file ijcai17.sty is the style file for IJCAI-17 (same as ijcai07.sty).
\usepackage{ijcai17}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{amsmath}
% Use the postscript times font!
\usepackage{times}
\usepackage{graphicx} 
\usepackage[ruled,vlined]{algorithm2e}

% the following package is optional:
%\usepackage{latexsym}


\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]

\title{Motif-Aware Graph Embeddings}

\author{Anonymous authors}

\begin{document}

\maketitle

\begin{abstract}
  In this paper, we propose two motif-aware approaches for the unsupervised 
  and semi-supervised graph embedding task. Our first model applies the most
  significant motif pattern on a graph as a guilding pattern for random walks.
  We then use a skipgram model with noise contrastive estimation to learn the
  graph embedding from generated random walk context. The second model employs the 
  higher-order organization (i.e. motifs organization) of complex networks, 
  and injects the higher-order connectivity patterns into each layer in a deep 
  graph convolutional networks. We demonstrate the effectiveness of our 
  motif-aware approaches on node labels classification, link prediction, and t-SNE 
  visuallization.
\end{abstract}

\section{Introduction}

\subsection{Complex network and machine learning}

Network modeling have been an essential tool for a wide
range of scientific fields \cite{physicnet,molecule,youtube,motifblockmilo,juremotif}.
The network science view usually reveals the underlying structure 
of a complex system. Based on the system's network structure, 
scientists can make predictions and explaination
about the system's behavior. For example, in biology, the
study on neuronal systems connectivity indicated
that the component arrangement of a neural system is optimized
for short processing paths rather than wiring lengths \cite{kaiser2006nonoptimal}. 
Similarly, social networks analysis provides communities structures
as well as social interaction patterns \cite{west2014exploiting,barabasi2014network}. 
However, along with the information explosion, the large graph-structured
data poses a great challenge for traditional network analysis methods
in term of scalability and complexity. To deal with such challendges,
one promising approach is to apply machine learning methods (especially
deep learning) methods to network problems.

Bridging the gap between network science and machine learning
is also a challenging task. Due to the irregularity in network and 
graph-structured data, it is desirable to have a \emph{meaningful}
and structural network representation for machine learning application. 
Traditionally, vector representation can be obtained via graph spectral methods.
However, spectral methods are shown to be unscalable without estimation
methods TODO: find theoretical citation \cite{deepwalk,node2vec}.
Recently, inspired by the skipgram model in natural language processing
\cite{skipgram}, \citeauthor{deepwalk} propsed their scalable graph
embedding algorithm named DeepWalk. Their results node classification
proved the effectiveness of their algorithm in learning a lower dimensionality 
representation of a complex network. Subsequence works to DeepWalk
further improved node classification accuracy by modifying graph 
context generation process \cite{line,grarep,node2vec}. On the other
hand, more direct (and more effective) approaches were proposed in
\cite{planetoid,gcn}. Instead of learning the network representation
using only network structure (e.g. adjacency matrix), \citeauthor{planetoid}
proposed to injects the known labeling and node feature into the
representation learning process. \citeauthor{gcn} further improved
results from planetoid \cite{planetoid} by applying graph convolution
technique in their deep network model. These aforementioned approaches 
are similar in the sense that they all learn a latent representation 
of a complex network from data, then use this representation to solve 
a network problem using various machine learning tools. 

\subsection{Motifs in complex network}

There are three scale of network analysis: macroscopic, mesoscopic, 
and microscopic. The macroscopic scale displays a network as a whole to
study its robustness \cite{callaway2000network} or dynamics TODO: find citation \cite{barabasi2014network}.
In contrast, the microscopic scale studies the pair-wise interactions
between nodes in a network which is specific to the given system
TODO: find citation \cite{physicnet}. On the other hand, the mesoscopic
scale is an intermediate in which we consider the network is a composition of
subgraphs. In many research, especially computational biology, the
mesoscopic components are called \emph{motifs}, and it is common
to think of them as building blocks for a complex system \cite{motifblockmilo}.

\begin{definition}{\emph{Network motif}}
Given a graph $G = {V,E}$, define a subgraph $G' = {V', E'}$ with $V' \subseteq V$;
$E' \subset E$ s.t. $i,j \in V' \forall e_{ij} \in E'$ and $|V'| \ll |V|$. Recurring subgraphs
are called \emph{network motif} when they are statistically significant.
\end{definition}

Also refered as higher-order organization by \citeauthor{juremotif}, network motifs
are believed to represent the underlying mechanism of a complex system 
\cite{netmotif,alon2006introduction,mangan2003structure}. 
For instance, the directional bi-fan motif TODO: figure
and its simplified undirectional version TODO: figure are crucial in a citation network. 
Beside having a statistical significance, bi-fan motif is also intuitively 
sensible in citation network as it represents the citation mechanism as an activity
in a subgraph. The correlation of recurring subgraphs and system functionality has 
been studied extensively in biological systems such as 
transcription networks \cite{mangan2003structure} and brain 
networks \cite{brainnetheuvel,honey2007network}. As networks motifs
have been recognized as the fundamental building block of a complex
systems, using them as a strucutural guidance for machine learning
on graph data can yield possitive improvements.

Generally, algorithms involving network motifs have to deal with
the problem of graph isomorphism. For such reason, in most analysis,
only motifs of size 5 or smaller are considered. In this paper,
we only consider motif of size 4 at most. This limitation is due to
the large size of networks that we experimented. Although limited 
by the motif size, we have been able to practically show the effectiveness of
the motif-aware methods. On the other hand, as mentioned in \cite{juremotif},
motif algorithms can be easily parallelized. Therefore, the extension to
larger size motifs can be made possible by parallelize the motif
analysis procedures. Further discussion will be provided in later sections.

\section{Methods}

In this section, we present the detail of our methods. Firstly,
we propose the basis for the network motif selection from a network.
Secondly, we present two approaches employing motif patterns to
learn graph embeddings: \emph{motifwalk} and \emph{m-gcn}.

\subsection{Network Motifs}

In the previous section, we have introduced the importance of
network motifs in network analysis. In this section, we present
the metric for measuring network motif significance and the definition
of motif laplacian.

In order to measure the importance of a network motif, we compare
the given network against a null model. The null model of an empirical network 
is an ensemble of randomly generated networks having the same number of nodes and
edges as the network. For small networks with less than 10,000 edges, we
generated 100 random networks as the ensemble of the null model. On the other
hand, we generated 10 random networks for the null model of larger networks.
The $z\mbox{-score}$ is given by:
\begin{equation*}
z\mbox{-score} = \frac{N_\mathbf{m}(G)-N_\mathbf{m}(G_{\scalebox{0.75}{random}})}{\sigma_\mathbf{m}(G_{\scalebox{0.75}{random}})}
\end{equation*}
where $N_\mathbf{m}(G)$ is the count of motif $\mathbf{m}$ in the empirical
network; $N_\mathbf{m}(G_{\scalebox{0.75}{random}})$ is the mean of the
null model; and $\sigma_\mathbf{m}(G_{\scalebox{0.75}{random}})$ is the variance.
The $z\mbox{-score}$'s values can range from $-\inf$ to $+\inf$. In practice,
the most simple motifs (figure \ref{fig:motif3}-m2,3,4) often have the highest
frequencies and negative $z\mbox{-score}$. We ignored such motifs in our analysis.
We select motif which has the highest positive $z\mbox{-score}$ and the highest frequency
as our motif of interest to construct the motif co-occurrence matrix.

The formal definition of the motif co-occurrence matrix for a
motif $\mathbf{m}$ on an unweighted, directed graph $G$ is given by:
$$M_{i,j} = \sum_{(v, \chi_{\mathcal{A}}(v)) \in \mathbf{m}} \mathbf{1}({i,j} \subset \chi_\mathcal{A}(v))$$
In here, $\mathcal{A}$ represents the anchor set; $(v, \chi_{\mathcal{A}}(v))$ 
represents pairs of node $v \in V_G$ and the other anchor nodes generated by $\chi_\mathcal{A}$.
If the anchor node set $\mathcal{A}$ is empty, all motif co-occureence is
counted toward the motif co-occurrence matrix $M$. Otherwise, only nodes in
the anchor set will be counted. Figure \ref{fig:anchor} illustrates
the bi-fan motif and an anchor set.

Generally, in this paper we employ the motif co-occurrence matrix
as: 1. An adjacency matrix describing a motif graph; 2. An adjacency
matrix from which we computes the Fourier basis for the graph convolution
operation. The first approach is straight forward as we want to generate
a network context where nodes occur in the motif pattern. For such reason,
we treat the motif co-occurence matrix as a binary matrix describing a new
network. On the other hand, the graph convolution approximation methods 
proposed in \cite{gcn,defferrard2016convolutional} only apply to symmetric 
binary matrices. In our model, the motif co-occurence matrix is a symmetric 
weighted matrix. The eigenvalue decomposition of such matrix is given by:
\begin{equation} \label{eq:eigm}
\mathcal{L}_{\mathbf{m}} = U_{\mathbf{m}} \Lambda_{\mathbf{m}} U^{\top}_{\mathbf{m}}
\end{equation}
where $\mathcal{L}_{\mathbf{m}} = D_{\mathbf{m}} - A_{\mathbf{m}}$; $U_{\mathbf{m}}$ is the
orthogonal basis (also called the Fourier basis in graph convolutional context);
and $\Lambda_{\mathbf{m}} = diag(\lambda_{\mathbf{m}})$.

\begin{algorithm}[h] \label{al:madj}
\KwData{Graph $G = (V,E)$}
\KwIn{binary, $\mathbf{m}$, $\mathcal{A}$}
\KwOut{context}
\Begin{
    context $\leftarrow$ [~]; \\
    $V$ $\leftarrow$ G.nodes(); \\
    nodes $\leftarrow$ Shuffle($V$); \\
    \For{node $\in$ nodes} {
        walks $\leftarrow$ [~]; \\
        \For{i=0; i<nwalk; ++i} {
            walks += RandomWalk(graph=$G$,start=node, len=length)
        }
        \For{i=0; i<nwalk; ++i} {
            walks += RandomWalk(graph=$G_{\mathbf{m}}$start=node, len=length)
        }
        context += walks
    }
\KwRet{context}
}
\caption{Motif co-occurrence matrix generation}
\end{algorithm}

TODO: Algorithms and estimation techniques. Discuss about
the change in maximum eigenvalue due to the weighted
motif laplacian.

\subsection{Biased Random Walk}

Previous skipgram-based graph embedding models employ random
walks for graph context generation. To improve the embedding results,
structure-aware context generation methods were proposed in \cite{line,node2vec}.
However, the limitation of \emph{LINE} lies at the fact that it only
consider the second-order proximity (bi-fan motif),  \emph{node2vec} requires
the costly cross-validation search for its hyperparameters $p$ and $q$.
To solve the above mentioned problems, we propose a biased random walk
algorithm for graph context generation which can be considered the 
generalization of \emph{LINE} and \emph{deepwalk}. Since our algorithm
decides the walk pattern supported by the most significant network motif before 
performing context generation, we achieve the simplicity of \emph{deepwalk}
while having the structure-aware context as of \emph{LINE} and \emph{node2vec}.

Our \emph{motifwalk} algorithm has two steps: motif adjacency matrix
construction and context generation. Firstly, we construct a binary
motif co-occurence matrix from the given network. We select the motif
pattern as described in the previous section. Since the constructed matrix 
accounts the co-occurrence of network node pairs in a motif, it is a symetric
matrix. Secondly, after having a second adjacency matrix describing the
motif structure, we run random walks on this new network for context generation. 
The obtained context is used jointly with random walks context generated with 
the original network to train an embedding skipgram model. Algorithm \ref{al:madj}
and algorithm \ref{al:cgen} describle the \emph{motifwalk} algorithm.
\begin{algorithm}[h] \label{al:cgen}
\KwData{Graph $G = (V,E)$, Motif Graph $G_{\mathbf{m}} = (V,E)$}
\KwIn{length, nwalk, nmwalk}
\KwOut{context}
\Begin{
    context $\leftarrow$ [~]; \\
    $V$ $\leftarrow$ G.nodes(); \\
    nodes $\leftarrow$ Shuffle($V$); \\
    \For{node $\in$ nodes} {
        walks $\leftarrow$ [~]; \\
        \For{i=0; i<nwalk; ++i} {
            walks += RandomWalk(graph=$G$,start=node, len=length)
        }
        \For{i=0; i<nwalk; ++i} {
            walks += RandomWalk(graph=$G_{\mathbf{m}}$start=node, len=length)
        }
        context += walks
    }
\KwRet{context}
}
\caption{Motif-aware graph context generation}
\end{algorithm}

\subsection{Motif Convolutional Architecture}

In this section we propose our motif convolutional deep
neural network architecture for semi-supervised graph labeling
tasks. Graph convolution is a signal processing technique in which
a network of signals reside on nodes (e.g. sensor network) is
processed in the graph spectral domain defined on the graph structure.
The convolution on a graph $G$ of a function of the graph Laplacian $g_{\theta}$ 
(also called a filter or a kernel) and a signal $x$ is defined as:
$$g_{\theta} \ast x = U g_{\theta} U^{\top} x,$$
where $L = U \Lambda U^\top$, $U$ is the Fourier basis
and $\Lambda$ is called the frequencies of the graph. 
Graph convolution has been shown effective in processing
graph-structured data, and also argued to be the generalization
of convolutional networks \cite{shuman2013emerging,defferrard2016convolutional,gcn}.
In practice, given a graph where each node has a feature vector,
we can treat the feature vector of the graph as signals. The output $y$
of these "signals" filtered by $g_\theta$ on the graph is given by
the graph convolution and deconvolution operations: 
\begin{equation}
\label{eq:1}
y = g_\theta (U \Lambda U^\top) x = U (g_\theta(\Lambda) U^\top)x
\end{equation}

Computing equation \ref{eq:1} is computationally expensive
due to the matrix multiplication and eigenvector decomposition operations.
Therefore, fast estimation methods such as Chebyshev polynomial was suggested
in \cite{hammond2011wavelets}. Based on the further linear approximation
proposed by \citeauthor{gcn}, we propose our two layers motif convolutional network
model as:
\begin{equation} \label{eq:2}
    \begin{aligned}
    Z_{\scalebox{0.75}{forward}} &= f(X,A,M) \\
    &= \mbox{softmax}(\hat{M} \mbox{ReLU}(\hat{A}XW^{(0)})W^{(1)}),
    \end{aligned}
\end{equation}
where $A$ and $M$ is a binary adjacency matrix and motif co-occurrence
matrix respectively; $\hat{A}$ and $\hat{M}$ are constructed by the
\emph{renormalization trick} as suggested in \cite{gcn}; $X$ contains
the feature vectors for each graph node; $W^{(0)}$ and $W^{(1)}$ are
learnable variables. With the backpropagation learning algorithm, the
weight of layer $k$ is updated as follow: 
\begin{equation}
\label{eq:3}
\frac{\partial E}{\partial W^{(k)}_{i,j}} = \sum^S_{s=1} [x]^\top \frac{\partial E}{\partial y_{k,j}}
\end{equation}

\section{Experiments}



\subsection{Datasets and observations}

\subsection{Motif significance}

\section{Results}

\subsection{Unsupervised}

Traditional task on blogcatalog and others.
Link prediction.
t-SNE.

\subsection{Semi-supervised}

Task on featured networks.

\section{Related work}

\subsection{Spectral approaches}

\subsection{Skipgram-based approaches }

\subsection{Deep neural network approaches}

\section{Discussion}

Our paper's contributions are proposing an extension to the graph convolutional 
architecture; proposing the uses and demonstrate the importance of motifs in
real worldnetworks.

Limitation: 


\section*{Acknowledgments}

I would like to thank.

%% The file named.bst is a bibliography style file for BibTeX 0.99c
\bibliographystyle{named}
\bibliography{ijcai17}

\end{document}

