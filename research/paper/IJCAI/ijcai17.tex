%%%% ijcai17.tex

\typeout{Motif-Aware Graph Embeddings}

% These are the instructions for authors for IJCAI-17.
% They are the same as the ones for IJCAI-11 with superficical wording
%   changes only.


\documentclass{article}
% The file ijcai17.sty is the style file for IJCAI-17 (same as ijcai07.sty).
\usepackage{ijcai17}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{amsmath}
% Use the postscript times font!
\usepackage{times}

% the following package is optional:
%\usepackage{latexsym}


\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]

\title{Motif-Aware Graph Embeddings}

\author{Anonymous authors}

\begin{document}

\maketitle

\begin{abstract}
  In this paper, we propose two motif-aware approaches for the unsupervised 
  and semi-supervised graph embedding task. Our first model applies the most
  significant motif pattern on a graph as a guilding pattern for random walks.
  We then use a skipgram model with noise contrastive estimation to learn the
  graph embedding from generated random walk context. The second model employs the 
  higher-order organization (i.e. motifs organization) of complex networks, 
  and injects the higher-order connectivity patterns into each layer in a deep 
  graph convolutional networks. We demonstrate the effectiveness of our 
  motif-aware approaches on node labels classification, link prediction, and t-SNE 
  visuallization.
\end{abstract}

\section{Introduction}

\subsection{Complex network and machine learning}

Network modeling have been an essential tool for a wide
range of scientific fields \cite{physicnet,molecule,youtube,motifblockmilo,juremotif}.
The network science view usually reveals the underlying structure 
of a complex system. Based on the system's network structure, 
scientists can make predictions and explaination
about the system's behavior. For example, in biology, the
study on neuronal systems connectivity indicated
that the component arrangement of a neural system is optimized
for short processing paths rather than wiring lengths \cite{kaiser2006nonoptimal}. 
Similarly, social networks analysis provides communities structures
as well as social interaction patterns \cite{west2014exploiting,barabasi2014network}. 
However, along with the information explosion, the large graph-structured
data poses a great challenge for traditional network analysis methods
in term of scalability and complexity. To deal with such challendges,
one promising approach is to apply machine learning methods (especially
deep learning) methods to network problems.

Bridging the gap between network science and machine learning
is also a challenging task. Due to the irregularity in network and 
graph-structured data, it is desirable to have a \emph{meaningful}
and structural network representation for machine learning application. 
Traditionally, vector representation can be obtained via graph spectral methods.
However, spectral methods are shown to be unscalable without estimation
methods TODO: find theoretical citation \cite{deepwalk,node2vec}.
Recently, inspired by the skipgram model in natural language processing
\cite{skipgram}, \citeauthor{deepwalk} propsed their scalable graph
embedding algorithm named DeepWalk. Their results node classification
proved the effectiveness of their algorithm in learning a lower dimensionality 
representation of a complex network. Subsequence works to DeepWalk
further improved node classification accuracy by modifying graph 
context generation process \cite{line,grarep,node2vec}. On the other
hand, more direct (and more effective) approaches were proposed in
\cite{planetoid,gcn}. Instead of learning the network representation
using only network structure (e.g. adjacency matrix), \citeauthor{planetoid}
proposed to injects the known labeling and node feature into the
representation learning process. \citeauthor{gcn} further improved
results from planetoid \cite{planetoid} by applying graph convolution
technique in their deep network model. These aforementioned approaches 
are similar in the sense that they all learn a latent representation 
of a complex network from data, then use this representation to solve 
a network problem using various machine learning tools. 

\subsection{Motifs in complex network}

There are three scale of network analysis: macroscopic, mesoscopic, 
and microscopic. The macroscopic scale displays a network as a whole to
study its robustness \cite{callaway2000network} or dynamics TODO: find citation \cite{barabasi2014network}.
In contrast, the microscopic scale studies the pair-wise interactions
between nodes in a network which is specific to the given system
TODO: find citation \cite{physicnet}. On the other hand, the mesoscopic
scale is an intermediate in which we consider the network is a composition of
subgraphs. In many research, especially computational biology, the
mesoscopic components are called \emph{motifs}, and it is common
to think of them as building blocks for a complex system \cite{motifblockmilo}.

\begin{definition}{\emph{Network motif}}
Given a graph $G = {V,E}$, define a subgraph $G' = {V', E'}$ with $V' \subseteq V$;
$E' \subset E$ s.t. $i,j \in V' \forall e_{ij} \in E'$ and $|V'| \ll |V|$. Recurring subgraphs
are called \emph{network motif} when they are statistically significant.
\end{definition}

Also refered as higher-order organization by \citeauthor{juremotif}, network motifs
are believed to represent the underlying mechanism of a complex system 
\cite{netmotif,alon2006introduction,mangan2003structure}. 
For instance, the directional bi-fan motif TODO: figure
and its simplified undirectional version TODO: figure are crucial in a citation network. 
Beside having a statistical significance, bi-fan motif is also intuitively 
sensible in citation network as it represents the citation mechanism as an activity
in a subgraph. The correlation of recurring subgraphs and system functionality has 
been studied extensively in biological systems such as 
transcription networks \cite{mangan2003structure} and brain 
networks \cite{brainnetheuvel,honey2007network}. As networks motifs
have been recognized as the fundamental building block of a complex
systems, using them as a strucutural guidance for machine learning
on graph data can yield possitive improvements.

Generally, algorithms involving network motifs have to deal with
the problem of graph isomorphism. For such reason, in most analysis,
only motifs of size 5 or smaller are considered. In this paper,
we only consider motif of size 4 at most. This limitation is due to
the large size of networks that we experimented. Although limited 
by the motif size, we have been able to practically show the effectiveness of
the motif-aware methods. On the other hand, as mentioned in \cite{juremotif},
motif algorithms can be easily parallelized. Therefore, the extension to
larger size motifs can be made possible by parallelize the motif
analysis procedures. Further discussion will be provided in later sections.

\section{Methods}

In this section, we present the detail of our methods. Firstly,
we propose the basis for the network motif selection from a network.
Secondly, we present two approaches employing motif patterns to
learn graph embeddings: \emph{motifwalk} and \emph{m-gcn}.

\subsection{Network Motifs}

In the previous section, we have introduced the importance of
network motifs in network analysis. In this section, we present
the metric for measuring network motif significance and the definition
of motif laplacian.

In order to test the significant of a network motif on the given
network, we use Z-score as our metric. Given a network which structure
can be described as a graph $G(V,E)$,

The formal definition of the motif co-occurrence matrix for a
motif $\mathbf{m}$ on an unweighted, directed graph $G$ is given by:
$$M_{i,j} = \sum_{(v, \chi_{\mathcal{A}}(v)) \in \mathbf{m}} \mathbf{1}({i,j} \subset \chi_\mathcal{A}(v))$$
In here, $\mathcal{A}$ represents the anchor set; $(v, \chi_{\mathcal{A}}(v))$ 
represents pairs of node $v \in V_G$ and the other anchor nodes generated by $\chi_\mathcal{A}$.
If the anchor node set $\mathcal{A}$ is empty, all motif co-occureence is
counted toward the motif co-occurrence matrix $M$. Otherwise, only nodes in
the anchor set will be counted. Figure \ref{fig:anchor} illustrates
the bi-fan motif and an anchor set.

TODO: Present laplacian and power of laplacian of graph. Present motif 
types and laplacian for each type.
Algorithms and estimation techniques. Discuss about
the change in maximum eigenvalue due to the weighted 
motif laplacian.

\subsection{Biased Random Walk}

Present the original motifwalk model.

\subsection{Motif Convolutional Architecture}

In this section we propose our motif convolutional deep
neural network architecture for semi-supervised graph labeling
tasks. Graph convolution is a signal processing technique in which
a network of signals reside on nodes (e.g. sensor network) is
processed in the graph spectral domain defined on the graph structure.
The convolution on a graph $G$ of a function of the graph Laplacian $g_{\theta}$ 
(also called a filter or a kernel) and a signal $x$ is defined as:
$$g_{\theta} \ast x = U g_{\theta} U^{\top} x,$$
where $L = U \Lambda U^\top$, $U$ is the Fourier basis
and $\Lambda$ is called the frequencies of the graph. 
Graph convolution has been shown effective in processing
graph-structured data, and also argued to be the generalization
of convolutional networks \cite{shuman2013emerging,defferrard2016convolutional,gcn}.
In practice, given a graph where each node has a feature vector,
we can treat the feature vector of the graph as signals. The output $y$
of these "signals" filtered by $g_\theta$ on the graph is given by
the graph convolution and deconvolution operations: 
\begin{equation}
\label{eq:1}
y = g_\theta (U \Lambda U^\top) x = U (g_\theta(\Lambda) U^\top)x
\end{equation}

Computing equation \ref{eq:1} is computationally expensive
due to the matrix multiplication and eigenvector decomposition operations.
Therefore, fast estimation methods such as Chebyshev polynomial was suggested
in \cite{hammond2011wavelets}. Based on the further linear approximation
proposed by \citeauthor{gcn}, we propose our two layers motif convolutional network
model as:
\begin{equation} \label{eq:2}
    \begin{aligned}
    Z_{\mbox{forward}} &= f(X,A,M) \\
    &= \mbox{softmax}(\hat{M} \mbox{ReLU}(\hat{A}XW^{(0)})W^{(1)}),
    \end{aligned}
\end{equation}
where $A$ and $M$ is a binary adjacency matrix and motif co-occurrence
matrix respectively; $\hat{A}$ and $\hat{M}$ are constructed by the
\emph{renormalization trick} as suggested in \cite{gcn}; $X$ contains
the feature vectors for each graph node; $W^{(0)}$ and $W^{(1)}$ are
learnable variables. With the backpropagation learning algorithm, the
weight of layer $k$ is updated as follow: 
\begin{equation}
\label{eq:3}
\frac{\partial E}{\partial W^{(k)}_{i,j}} = \sum^S_{s=1} [x]^\top \frac{\partial E}{\partial y_{k,j}}
\end{equation}

\section{Experiments}



\subsection{Datasets and observations}

\subsection{Motif significance}

\subsection{Architectures}

\section{Results}

\subsection{Unsupervised}

Traditional task on blogcatalog and others.
Link prediction.
t-SNE.

\subsection{Semi-supervised}

Task on featured networks.

\section{Related work}

\subsection{Spectral approaches}

\subsection{Skipgram-based approaches }

\subsection{Deep neural network approaches}

\section{Discussion}

Our paper's contributions are proposing an extension to the graph convolutional 
architecture; proposing the uses and demonstrate the importance of motifs in
real worldnetworks.

Limitation: 


\section*{Acknowledgments}

I would like to thank.

%% The file named.bst is a bibliography style file for BibTeX 0.99c
\bibliographystyle{named}
\bibliography{ijcai17}

\end{document}

