% Author: Hoang NT
% Created: 2016-06-26

\documentclass{sig-alternate-05-2015}

\newdef{definition}{Definition}
\usepackage{amsmath, amsfonts, amssymb}
\usepackage{dsfont}
\usepackage[ruled,vlined]{algorithm2e}

%\tolerance=1
%\emergencystretch=\maxdimen
%\hyphenpenalty=10000
%\hbadness=10000

\begin{document}

% Copyright
\setcopyright{acmcopyright}
%\setcopyright{acmlicensed}
%\setcopyright{rightsretained}
%\setcopyright{usgov}
%\setcopyright{usgovmixed}
%\setcopyright{cagov}
%\setcopyright{cagovmixed}


% DOI
\doi{10.475/123_4}

% ISBN
\isbn{123-4567-24-567/08/06}

%Conference
\conferenceinfo{WSDM '17}{February 2--9, 2017, Cambridge, UK}

\acmPrice{\$15.00}

%
% --- Author Metadata here ---
\conferenceinfo{Web Search and Data Mining}{'17 Cambridge, USA}
%\CopyrightYear{2007} % Allows default copyright year (20XX) to be over-ridden - IF NEED BE.
%\crdata{0-12345-67-8/90/01}  % Allows default copyright data (0-89791-88-6/97/05) to be over-ridden - IF NEED BE.
% --- End of Author Metadata ---

\title{Motif-Aware Graph Embedding
%\titlenote{Funded by Japanese Governement.}
%\subtitle{[Extended Abstract]
%\titlenote{A full version of this paper is available as
%\textit{Author's Guide to Preparing ACM SIG Proceedings Using
%\LaTeX$2_\epsilon$\ and BibTeX} at
%\texttt{www.acm.org/eaddress.htm}}
}
%
% You need the command \numberofauthors to handle the 'placement
% and alignment' of the authors beneath the title.
%
% For aesthetic reasons, we recommend 'three authors at a time'
% i.e. three 'name/affiliation blocks' be placed beneath the title.
%
% NOTE: You are NOT restricted in how many 'rows' of
% "name/affiliations" may appear. We just ask that you restrict
% the number of 'columns' to three.
%
% Because of the available 'opening page real-estate'
% we ask you to refrain from putting more than six authors
% (two rows with three columns) beneath the article title.
% More than six makes the first-page appear very cluttered indeed.
%
% Use the \alignauthor commands to handle the names
% and affiliations for an 'aesthetic maximum' of six authors.
% Add names, affiliations, addresses for
% the seventh etc. author(s) as the argument for the
% \additionalauthors command.
% These 'additional authors' will be output/set for you
% without further effort on your part as the last section in
% the body of your article BEFORE References or any Appendices.

\numberofauthors{3} %  in this sample file, there are a *total*
% of EIGHT authors. SIX appear on the 'first-page' (for formatting
% reasons) and the remaining two appear in the \additionalauthors section.
%
\author{
% You can go ahead and credit any number of authors here,
% e.g. one 'row of three' or two rows (consisting of one row of three
% and a second row of one, two or three).
%
% The command \alignauthor (no curly braces needed) should
% precede each author name, affiliation/snail-mail address and
% e-mail address. Additionally, tag each line of
% affiliation/address with \affaddr, and tag the
% e-mail address with \email.
%
% 1st. author
\alignauthor
Hoang Nguyen\\ %\titlenote{Hoang Nguyen is a master student.}\\
       \affaddr{Tokyo Institute of Technology}\\
       \email{hoangnt@ai.cs.titech.ac.jp}
\alignauthor
Nukui Shun\\ 
       \affaddr{Tokyo Institute of Technology}\\
       \email{nukui.s@net.c.titech.ac.jp}
% 2nd. author
\alignauthor
Tsuyoshi Murata\\ %\titlenote{Prof. Murata is the supervisor}\\
       \affaddr{Tokyo Insitute of Techonology}\\
       \email{murata@c.titech.ac.jp}
}
% There's nothing stopping you putting the seventh, eighth, etc.
% author on the opening page (as the 'third row') but we ask,
% for aesthetic reasons that you place these 'additional authors'
% in the \additional authors block, viz.
%\additionalauthors{Additional authors: John Smith (The Th{\o}rv{\"a}ld Group,
%email: {\texttt{jsmith@affiliation.org}}) and Julius P.~Kumquat
%(The Kumquat Consortium, email: {\texttt{jpkumquat@consortium.net}}).}
\date{26 June 2016}
% Just remember to make sure that the TOTAL number of authors
% is the number that will appear on the first page PLUS the
% number that will appear in the \additionalauthors section.

\maketitle
\begin{abstract}
Given a large complex graph, how can we learn a lower dimension
vector representation of each vertex that preserves structural 
information? Recent advancements in graph embedding have used
word embedding techniques and deep architectures to propose a
feasible answer to this question. However, most of these work
considers the notion of ``neighborhood'' by node adjacency only.
In this paper, we propose a novel graph embedding algorithm that
employs motif structures into the latent vector representation 
learning process. By contrasting between sets of nodes created 
by random walks and sets of nodes created by biased \emph{motif walk},
we show that embedding results of our algorithm are more
accurate in various benchmark graph mining tasks compared to
existing algorithms. The source code and results of our algorthms
is available online at \url{https://github.com/anonsyuushi/mage.git}.
\end{abstract}



%
% Generated CCS code: dl.acm.org/ccs/ccs.cfm
%

\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10010147.10010257.10010293.10010319</concept_id>
<concept_desc>Computing methodologies~Learning latent representations</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10010147.10010257.10010293.10010294</concept_id>
<concept_desc>Computing methodologies~Neural networks</concept_desc>
<concept_significance>300</concept_significance>
</concept>
<concept>
<concept_id>10002950.10003624.10003633</concept_id>
<concept_desc>Mathematics of computing~Graph theory</concept_desc>
<concept_significance>300</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Computing methodologies~Learning latent representations}
\ccsdesc[300]{Computing methodologies~Neural networks}
\ccsdesc[300]{Mathematics of computing~Graph theory}


%
% End generated code
%

%
%  Use this command to print the description
%
\printccsdesc

% We no longer use \terms command
%\terms{Theory}

\keywords{Distributed representation; Graph embedding; motif; auto-encoder; word2vec; MAGE}

\section{Introduction}

Meaningful distributed representation of a high dimensional sparse
dataset has proven to be useful for various machine learning tasks.
For example, the \textbf{dense vector representation} of words in 
word2vec framework \cite{w2v} has enabled machine learning
researchers to \textbf{put applications and citation of word2vec here}.

Recently, starting from 2014 with the \emph{DeepWalk} algorithm \cite{deepwalk},
there has been many proposed algorithms to encode a network's component such as
vertices or edges into a high dimensional real vector. The motivation behind
these algorithms is to learn a dense representation of the network anologous 
to learning a dense representation of a word \cite{w2v}. By encoding the 
whole network into vectors, graph embedding algorithms have
enabled network researcher to use the power of neural network techniques on
network data \cite{all_deep_learning_on_network}. Basically this solve the
sparsity problem of the network.

Our algorithm outperfroms every other algorithms on their test.
We conducted experiment throughly and carefully. Each experiment is ran
with the same condition 10 times and take average, we have all the deviation
stuff that others don't have. Really, it's really a very good paper.
Please accept it so I can go to UK for a vacation. Please.

The good news is, with only a handful of manual
settings\footnote{Two of these, the {\texttt{\char'134 numberofauthors}}
and {\texttt{\char'134 alignauthor}} commands, you have
already used; another, {\texttt{\char'134 balancecolumns}}, will
be used in your very last run of \LaTeX\ to ensure
balanced column heights on the last page.}, the \LaTeX\ document
class file handles all of this for you.

The remainder of this document is concerned with showing, in
the context of an ``actual'' document, the \LaTeX\ commands
specifically available for denoting the structure of a
proceedings paper, rather than with giving rigorous descriptions
or explanations of such commands.

\section{Related work}

In the context of topological graph theory, an embedding refers to a
representation of a graph $G$ on a surface $\Sigma$. Graph embedding
can also be viewed as dimension reduction when the dimensionality
of the surface $\Sigma$ is less than the dimensionality of the graph.
Meaningful graph embedding, in practice, is low dimensionality vector
representations of vertices preserving some analytical properties of
the graph. Generally, there are two main approaches to obtain high-quality
graph embeddings: affinity matrix factorization and machine learning
with neural networks.

\subsection{Matrix factorization}

== NUKUI ==

\subsection{Skipgram model}

Our work in this paper is directly related to
the \emph{word2vec} model \cite{w2c}. More specifically,
the 

The literature of graph embedding start with
matrix decomposition techniques \cite{la}. Super
professor et. al. perform amazing works
and achieved many success. Traditionally, graph
clustering procedure where similarity between
nodes or edges is defined and group together
by some similarity metric. This leads to the
notation of closeness and hence community.
However, one limitation of these classical 
graph embedding technique is that it depends 
on matrix decompositon which is very computational 
expensive.

Recently, with the emerging of deep neural network
models such as autoencoder \cite{la}, restricted 
Bolzman machine \cite{la}, and some other that I will
fill here later. Inspired by these advancement of
natural language processing neural network models,
Peperozi et. al. \cite{deepwalk} proposed a graph
embedding framework named DeepWalk. By taking advantage
of the network structure and create an artificial
``node corpus'' by performing random walk, DeepWalk
has achieved good performance and inspired any
following researches. Since 2014, many graph embedding
model based on random walk has been proposed such as
LINE, GraRep, etc. These embedding algorithm proved
its efficiency and usefullness in various graph
mining tasks and classification problem in large graph.

Out of the aforementioned embedding algorithms,
LINE has taken our notice. LINE model improved
DeepWalk by incooporating the notation of second
order proximity to the learning task. This leads
to its outstanding performance without other auxilariy
information. There are some research that has 
good result in embedding too, but out of all research
that only use graph structure, LINE is the best.
We think that the key to LINE's success was
the second order proximity added to the original
embedding scheme. However, LINE perform extremely well
in network with citation-like structure. In other word,
LINE is very good for network with wedge motif.
From this observation, we think that if we can 
incooporate the most popular substructure of the
graph to the embedding scheme, we can learn better
vector representation.

\subsection{Motif in Graph}
Motif is defined as a small subgraph that has
some funny characteristic \cite{la}, maybe cite some
of Jure's work here.

\begin{definition}
A graph is represented as $G = (V,E)$, where $V$ is the
set of vertices and $E$ is the set of edges between vertices.
Each edges $e \in E$ is an ordered pair $e = (u,v)$ where
$u, v \in V$. A graph is called \emph{undirected} when
$(u,v) \equiv (v,u)$, and \emph{directed} when $(u,v) \not\equiv (v,u)$.
\end{definition}

\section{Motif-aware graph embedding}

As mentioned in the previous sessions, our work aims
to improve embedding quality by manipulating the graph
data generation process. We have two data generation processes in 
our framework. The first process uses conventional 
random walk and a ``skip window'' to generate positive
samples, while negative samples are picked from a noise
distribution $p_n(x)$. The second uses a pre-defined 
\emph{motif walk} to generate positive samples, while
negative samples is generated from a contrasting distribution:
\begin{equation} \label{eq:dist}
  p_{\scalebox{0.7}{mc}}(x) = 
    \boldsymbol f\left(p^t_{\scalebox{0.7}{m}}(x) \ || \ p^t_{\scalebox{0.7}{r}}(x)\right),
\end{equation}
where $\boldsymbol f(.)$ is chosen to be a distance function that
yields high probability for node $x$ when $x$ is more
likely to appear in random walk starting from node $t$
than in motif walk starting from node $t$. $p^t_m(x)$ and
$p^t_r(x)$ are distribution of nodes count in motif walk
and random walk starting from vertex $t$. 

Our model is a variant of the skipgram model \cite{w2v}
using the softmax function, in which we have the  
conditional probability of a ``class'' vertex (context) given 
the ``target'' vertex (word) as follow:
\begin{equation} \label{eq:softmax}
  p\left(v_{\scalebox{0.7}{class}} \ | \ v_{\scalebox{0.7}{target}}\right) 
    = \frac{\exp(\omega^\top_c \cdot \omega_t)}{\sum^{|V|}_{i=1} \exp(\omega^\top_i \cdot \omega_t)}
\end{equation}
Despite the efficiency and simplicity of this model,
the task of computing the normalization factor requires
summing all over the graph's vertices is intractable
for large graph. To solve this normalization problem,
estimation techiques such as hierachial softmax \cite{hsoftmax}, 
noise contrastive estimation \cite{nce}, and negative
sampling \cite{w2v} are employed in the recent graph
embedding models \cite{deepwalk, line, platenoid}.
We define the loss function with negative sampling
for our model as follow:

\begin{equation} \label{eq:loss}
  \begin{aligned}
    \mathcal{L} & = -\log p\left(v_{\scalebox{0.7}{class}} \ | \ v_{\scalebox{0.7}{target}}\right)
                  = \alpha \mathcal{L}_{\scalebox{0.7}{r}} + (1-\alpha) \mathcal{L}_{\scalebox{0.7}{mc}} \\
  \end{aligned}
\end{equation}

\begin{equation} \label{eq:loss_each}
  \begin{aligned}
    \mathcal{L}_{\scalebox{0.7}{r}} & = -\log \sigma \left(\omega^\top_c \cdot \omega_t\right) - 
      \sum^{k}_{i=1} \mathds{E}_{\omega_s \sim p_{\scalebox{0.6}{n}}(\omega)} \left[\sigma (-\omega^\top_s \cdot \omega_t)\right] \\
    \mathcal{L}_{\scalebox{0.7}{mc}} & = -\log \sigma \left(\omega^\top_c \cdot \omega_t\right) -
      \sum^{k}_{i=1} \mathds{E}_{\omega_s \sim p_{\scalebox{0.6}{mc}}(\omega)} \left[\sigma (-\omega^\top_s \cdot \omega_t)\right] \\
  \end{aligned}
\end{equation}

\subsection{Graph sampling process}

The first data generation process is similar to that of
the Deepwalk model. The difference in our model is negative
sampling from the vertex degree distribution is used to
estimate the valid distribution. Algorithm \ref{al:gen_rand}
describes the positive and negative samples generating 
from random walk process.

\begin{algorithm}[h] \label{al:gen_rand}
\KwData{Graph $G = (V,E)$}
\KwIn{walkLength, skipWindow, numSkip, numNeg, distort, \texttt{random\_walk}}
\KwOut{(targets, classes, labels)}
\Begin{
  targets $\leftarrow$ [~]; classes $\leftarrow$ [~];
  labels $\leftarrow$ [~]; \\
  idList $\leftarrow$ Shuffle($V$); \\
  \For{i $\in$ idList}{
    walk $\leftarrow$ \texttt{randomWalk}(start=i, length=walkLength);
    \For{j $\in$ walk}{
      \For{j $\in$ \texttt{range}(numSkip)}{
        targets.\texttt{append}(j); \\
        classes.\texttt{append}(\texttt{random.choice}(walk[j-skipWindow:j+skipWindow]); \\
        labels.\texttt{append}(1.0); \\
      }
      \For{j $\in$ \texttt{range}(numNeg)}{
        targets.\texttt{append}(j); \\
        classes.\texttt{append}(\texttt{random.choice}($V$, distort)); \\
        labels.\texttt{append}(0.0); \\
      }
    }
  }
\KwRet{(targets, classes, labels);}
}
\caption{\texttt{gen\_rand}: sample by random walk}
\end{algorithm}

The second data generation process is the core of
our method. Positive data samples are generated
using a biased random walk, which is called 
\texttt{motif\_walk}. For the negative samples
generation, we perform unbiased random walk and
select vertices that appear frequenty in the
random walk, but less frequenty in the postive
motif walk. The intuition for this technique comes
from our hypothesis that vertices in the same motif 
cluster are more likely to be related than vertices
in the random walk. As discussed by Gutmann and 
Hyv{\"a}rinen in \cite{nce} for the choice of noise
distribution in practice, the more similar the
noise distribution to the true distribution leads
to better learning result.

\begin{algorithm}[h] \label{al:gen_m}
\KwData{Graph $G = (V,E)$}
\KwIn{mwalkLength, rwalkLength, numSkip, numNeg, contrastIter, \texttt{motif\_walk}}
\KwOut{(targets, classes, labels)}
\Begin{
  targets $\leftarrow$ [~]; classes $\leftarrow$ [~];
  labels $\leftarrow$ [~]; \\
  idList $\leftarrow$ Shuffle($V$); \\
  \For{i $\in$ idList}{
    posSet $\leftarrow$ \{~\}; 
    negSet $\leftarrow$ \{~\}; \\
    \For{j $\in$ \texttt{range}(contrastIter)}{
      pwalk $\leftarrow$ \texttt{motif\_walk}(start=i, length=mwalkLength); \\
      posSet.\texttt{add}(pwalk); \\
      nwalk $\leftarrow$ \texttt{random\_walk}(start=i, length=rwalkLength); \\
      negSet.\texttt{add}(nwalk); \\
    }
    \For{k $\in$ \texttt{range}(numSkip)}{
      targets.\texttt{append}(j) \\
      classes.\texttt{append}(\texttt{random.choice}(posSet) \\
      labels.\texttt{append}(1.0)
    }
    \For{k $\in$ \texttt{range}(numNeg)}{
      targets.\texttt{append}(j) \\
      classes.\texttt{append}(\texttt{random.choice}(negSet - posSet))
      labels.\texttt{append}(0.0)
    }
  }
}
\KwRet{(targets, classes, labels)}
\caption{\texttt{gen\_motif}: sample by motif walk}
\end{algorithm}

Algorithm \ref{al:gen_m} generates positive samples
without the need of skip window for choosing local
vertices given the target vertex. For negative
samples, the vertices that apprear in random walk but
not in motif walk are chosen at random. In this algorithm
we have two free parameters: function \texttt{motif\_walk}
and the contrasting method $\boldsymbol f$. For simplicity,
we implement undirected triangle (algorithm \ref{al:mwalk}) 
and directed bipartite (algorithm \ref{al:mbiwalk})
motif walk with simple set subtraction for contrastive 
sampling. However, the extension is discussed in
later sections.

\begin{algorithm}[h] \label{al:mwalk}
\KwData{Graph $G = (V,E)$}
\KwIn{start, mwalkLength}
\KwOut{walk}
\Begin{
==
}
\KwRet{(targets, classes, labels)}
\caption{\texttt{triange\_walk}: triangle motif walk}
\end{algorithm}

\begin{algorithm}[h] \label{al:mbiwalk}
\KwData{Graph $G = (V,E)$}
\KwIn{mwalkLength, rwalkLength, numSkip, numNeg, contrastIter, \texttt{motif\_walk}}
\KwOut{(targets, classes, labels)}
\Begin{
==
}
\KwRet{(targets, classes, labels)}
\caption{\texttt{bipartite\_walk}: sample by motif walk}
\end{algorithm}

\subsection{Optimization}

Our objective is to minimize the log-loss described in 
equation \ref{eq:loss}, in which parameter $\alpha$ 
controls the training portiion of random walk samples
and motif walk samples. We employ binary cross entropy
as the batch loss in our implementation:

\begin{equation} \label{eq:xentropy}
  \mathcal{H}(p_{\scalebox{0.7}{~predicted}}, p_{\scalebox{0.7}{~label}}) 
    = -\sum_{x} p_{\scalebox{0.7}{~predicted}}(x) \log p_{\scalebox{0.7}{~label}}
\end{equation}

Figure \ref{fig:keras} describes our training framework.
Backpropagation is used to miminize the loss function
described in equation \ref{eq:loss} and equation 
\ref{eq:xentropy}. The parameter set of our models is 
$ \{\mathcal{W}_{\scalebox{0.7}{emb}}, \mathcal{W}_{\scalebox{0.7}{neg}} \} $,
each has shape $(|V|, d)$, where $d$ denotes the embedding dimension.
The parameters are initialized by normal distribution
and uniform distribution of mean 0. During training,
we linearly change the value of $\alpha$ starting from 1
to 0. As $\alpha$ value changes, our model is trained
on different portion of random walk samples and motif
walk samples.

\subsection{Training procedure}

\section{Experiments}

%ACKNOWLEDGMENTS are optional
\section{Acknowledgments}
We acknowledge our friends who gave us insightful input.

\bibliographystyle{abbrv}
\bibliography{2016_motifwalk}  


\end{document}
